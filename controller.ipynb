{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Reference branch: https://github.com/invisibleForce/ENAS-Pytorch/blob/master/controller_model.py\n",
    "\n",
    "This draft only implements the macro search of the algorithm. Given the amount of code here, I wonder if the scope to implement both Micro and Macro could be too big. We can always start with something small and make sure this part works first\n",
    "\n",
    "To-dos:\n",
    "1. Implement infrastructure functions - readImage\n",
    "2. The current code only give 4 choices of conv network -  I doubt the choices. We could figure out later and use different choices.\n",
    "3. Re-review controller related code and refactor \n",
    "4. put everything together\n",
    "5. Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Efficient Neural Architecture Search (ENAS), a \"child\" refers to a specific neural network architecture generated during the search process. ENAS is an approach to automate the design of neural networks, where a \"controller\" network generates potential architectures, known as \"child\" networks.\n",
    "\n",
    "Here's a more detailed breakdown:\n",
    "\n",
    "Controller Network: The controller is typically a recurrent neural network (RNN) that predicts a sequence of actions, each defining a component of the neural network architecture (e.g., type of layer, number of filters, connections).\n",
    "\n",
    "Child Network: Each sequence generated by the controller corresponds to a unique \"child\" network architecture. These child networks are trained and evaluated to measure their performance on a specific task. \n",
    "\n",
    "Performance Feedback: The performance of the child network (e.g., accuracy, loss) is fed back to the controller, which uses this information to improve its architecture generation process. This feedback loop helps the controller learn which architectural features lead to better performance.\n",
    "\n",
    "The ENAS process aims to find optimal or near-optimal neural network architectures efficiently by leveraging the controller's ability to learn and refine the architecture search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as f\n",
    "import time\n",
    "import random\n",
    "\n",
    "DEBUG = False\n",
    "SEED = 202407240122\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerOperation(nn.Module):\n",
    "    \"\"\"\n",
    "    An operation used by a nas layer\n",
    "    Note:\n",
    "        conv3/5: need to pad zeros to let the input and output \n",
    "        feature maps have the same size. The size padded zeros\n",
    "        are given as follows. \n",
    "        ofmap size:\n",
    "        E = np.floor((H + 2px - 1 * (R - 1) - 1) / Sx + 1) # see pytorch nn.conv2d for details\n",
    "        F = np.floor((H + 2px - 1 * (R - 1) - 1) / Sx + 1)\n",
    "        Let E = H, F = W, so we can solve px and py as follows\n",
    "        Height: px = (R - 1) / 2\n",
    "        Width: py = (P - 1) / 2\n",
    "    \"\"\"\n",
    "    # op: conv3, conv5, avgpool3, maxpool3\n",
    "    # out_channels: = M, num of filters\n",
    "    def __init__(self, operation, out_channels):\n",
    "        self.operation = operation\n",
    "        self.out_channels = out_channels\n",
    "        super(LayerOperation, self).__init__() \n",
    "        self.layer_list = self._build_layer()\n",
    "    \n",
    "    def _build_layer(self):\n",
    "        layers = []\n",
    "        conv_in = nn.Conv2d(\n",
    "            in_channels=self.out_channels, \n",
    "            out_channels=self.out_channels, \n",
    "            kernel_size = 1, \n",
    "            stride=1)\n",
    "        batch_norm = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "        relu = nn.ReLU()\n",
    "        kernel = self.get_defined_operation_kernel(self.operation)\n",
    "\n",
    "        layers.append(conv_in)\n",
    "        layers.append(batch_norm)\n",
    "        layers.append(relu)\n",
    "        layers.append(kernel)\n",
    "        \n",
    "        if (self.operation == 'conv3') or (self.operation == 'conv5'):\n",
    "            bn_out = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "            layers.append(bn_out)\n",
    "\n",
    "        layers = nn.ModuleList(layers)\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    def get_defined_operation_kernel(self, operation):\n",
    "        if operation == 'maxpool3':\n",
    "            padding_x = 1\n",
    "            padding_y = padding_x\n",
    "            padding_size = (padding_x, padding_y)\n",
    "            kernel = nn.MaxPool2d(kernel_size=3, padding=padding_size, stride=1)\n",
    "        elif operation == 'avgpool3':\n",
    "            padding_x = 1\n",
    "            padding_y = padding_x\n",
    "            padding_size = (padding_x, padding_y)\n",
    "            kernel = nn.AvgPool2d(kernel_size=3, padding=padding_size, stride=1)\n",
    "        elif operation == 'conv3':\n",
    "            padding_x = 1\n",
    "            padding_y = padding_x\n",
    "            padding_size = (padding_x, padding_y)\n",
    "            kernel = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=3, padding=padding_size, stride=1)\n",
    "        elif operation == 'conv5':\n",
    "            padding_x = 2\n",
    "            padding_y = padding_x\n",
    "            padding_size = (padding_x, padding_y)\n",
    "            kernel = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=5, padding=padding_size, stride=1)\n",
    "        else:\n",
    "            raise ValueError('operation not supported')\n",
    "        return kernel\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class NasLayer(nn.Module):\n",
    "    def __init__(self, out_channels=24):\n",
    "        self.out_channels = out_channels\n",
    "        super(NasLayer, self).__init__()\n",
    "        self.layers = self._build_nas_layer()\n",
    "        \n",
    "\n",
    "    # For the time concerns, we can reduce the search to only one conv and one pooling layer. That should reduce a big chunk of time.\n",
    "    def _build_nas_layer(self):\n",
    "        layers = []\n",
    "        conv3 = LayerOperation('conv3', self.out_channels)\n",
    "        conv5 = LayerOperation('conv5', self.out_channels)\n",
    "        avgpool3 = LayerOperation('avgpool3', self.out_channels)\n",
    "        maxpool3 = LayerOperation('maxpool3', self.out_channels)\n",
    "        bn_out = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "    \n",
    "        layers.append(conv3)\n",
    "        layers.append(conv5)\n",
    "        layers.append(avgpool3)\n",
    "        layers.append(maxpool3)\n",
    "        layers.append(bn_out)\n",
    "        layers = nn.ModuleList(layers)\n",
    "    \n",
    "        return layers\n",
    "\n",
    "    # not sure what it is doing\n",
    "    def layer_operation(self, x, op):\n",
    "        \"\"\"\n",
    "        Run the operation of a nas layer\n",
    "        Args:\n",
    "            x: ifmap\n",
    "            op: operation to run\n",
    "                0 - conv3\n",
    "                1 - conv5\n",
    "                2 - avgpool3\n",
    "                3 - maxpool3\n",
    "        Returns:\n",
    "            x: ofmap\n",
    "        \"\"\"\n",
    "        x = self.layers[op[0]](x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def skip(self, prev_layers, config):\n",
    "        \"\"\"\n",
    "        Concatenate the desired previous layers of a nas layer\n",
    "        Args:\n",
    "            prev_layers: previous layers\n",
    "            config: describe all the combined layers\n",
    "        Returns:\n",
    "            y: ofmap\n",
    "        \"\"\"\n",
    "        layer_index_offset = 1  # used to skip the root_node_conv\n",
    "        num_layer = len(prev_layers) - layer_index_offset\n",
    "\n",
    "        desired_layers= []\n",
    "        for i in range(num_layer):\n",
    "            if config[i]: # layer specific config, if layer is selected\n",
    "                desired_layers.append(prev_layers[i + layer_index_offset])\n",
    "\n",
    "        if len(desired_layers):\n",
    "            desired_layers = torch.stack(desired_layers) # stack all the tensors in an additional axis (i.e., 0)\n",
    "            desired_layers = torch.sum(desired_layers, dim=0) # add along axis 0\n",
    "        else:\n",
    "            # if no layer is selected, return a tensor of zeros of size root_node_conv\n",
    "            desired_layers = torch.zeros(prev_layers[0].size()) \n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                desired_layers = desired_layers.cuda()\n",
    "        \n",
    "        return desired_layers\n",
    "    \n",
    "\n",
    "\n",
    "    def __call__(self, cnt_layer, prev_layers, layer_config):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prev_layers: all previous layers\n",
    "            layer_config: op and connectivity\n",
    "        \"\"\"\n",
    "        x = prev_layers[-1]\n",
    "        operation_config = layer_config[0]\n",
    "        x = self.layer_operation(x, operation_config)\n",
    "\n",
    "        if cnt_layer > 0:\n",
    "            skip_config = layer_config[1]\n",
    "            y = self.skip(prev_layers, skip_config)\n",
    "            x = [x, y]\n",
    "            x = torch.stack(x)\n",
    "            x = torch.sum(x, dim=0)\n",
    "\n",
    "            x = self.layers[-1](x) # bn_out\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChildModel(nn.Module):\n",
    "    def __init__(self,\n",
    "               num_of_class,\n",
    "               num_layers=6,\n",
    "               out_channels=24,\n",
    "               batch_size=32\n",
    "              ):\n",
    "\n",
    "        super(ChildModel, self).__init__() \n",
    "\n",
    "        self.num_of_class = num_of_class \n",
    "        self.num_layers = num_layers # We should define this; it should be part of the fine tuning process\n",
    "        self.out_channels = out_channels\n",
    "        self.graph = self.build_graph(self.num_of_class)\n",
    "        \n",
    "\n",
    "    def build_graph(self, class_num):\n",
    "        \"\"\"\n",
    "        This method gives the visual representation of the model\n",
    "        stem_conv: [N, C, H, W] -> [N, 3, 24, 24]\n",
    "        kernel: [M, C, R, P] -> [M, 3, 3, 3] /[Sx=1, Sy=1] -> I bet we need to recalculate this\n",
    "        \"\"\"\n",
    "        graph = []\n",
    "\n",
    "        # Build root conv layer\n",
    "        root_node_conv = self.build_root_conv()\n",
    "        graph.append(root_node_conv)\n",
    "        for _ in range(self.num_layers):\n",
    "            graph.append(NasLayer(self.out_channels))\n",
    "        # fully connected layer\n",
    "        fc = nn.Linear(self.out_channels, class_num, bias=True)\n",
    "        graph.append(fc)\n",
    "\n",
    "        graph = nn.ModuleList(graph) # this will hold submodule in a list\n",
    "\n",
    "        return graph\n",
    "    \n",
    "    def build_root_conv(self):\n",
    "        px = 1   # px = int((3 - 1) / 2) # we need to figure out what does that 3 mean.....\n",
    "        py = px\n",
    "        padding_size = (px, py)\n",
    "        root_node_conv = nn.Conv2d(\n",
    "                in_channels=3, \n",
    "                out_channels=self.out_channels, \n",
    "                kernel_size = 3, \n",
    "                padding=padding_size,\n",
    "                stride=1)\n",
    "        return root_node_conv\n",
    "    \n",
    "    def global_avgpool(self, x):\n",
    "        \"\"\"\n",
    "        An operation used to reduce the H and W axis\n",
    "        x = [N, C, H, W] -> [N, C, 1, 1]\n",
    "        \"\"\"\n",
    "        H = x.size()[2]\n",
    "        W = x.size()[3]\n",
    "        x = torch.sum(x, dim=[2, 3])\n",
    "        x = x / (H * W)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def model(self, x, sample_arch):\n",
    "        \"\"\"\n",
    "        run (like forward) a child model determined by sample_arch\n",
    "        Use the given test to get a preview of sample architecture\n",
    "        Args:\n",
    "            sample_arch: a list consisting of 2 * num_layers elements\n",
    "                op_id = sample_arch[2k]: operation id\n",
    "                skip = sample_arch[2k + 1]: element i of such abinary vector \n",
    "                    is used to describe whether the previous layer i is used \n",
    "                    as an input\n",
    "            x: input of the child model\n",
    "        Return:\n",
    "            x: output of the child model\n",
    "        \"\"\"\n",
    "        # layers\n",
    "        prev_layers = []\n",
    "        # root_node_conv\n",
    "        x = self.graph[0](x)\n",
    "        prev_layers.append(x)\n",
    "        index_offset = 1\n",
    "        # nas_layers\n",
    "        for layer_index in range(self.num_layers):\n",
    "            layer_config = sample_arch[2 * layer_index : 2 * layer_index + 2]   # [op], [skip]\n",
    "            x = self.graph[layer_index + index_offset](layer_index, prev_layers, layer_config)\n",
    "            prev_layers.append(x)\n",
    "        x = self.global_avgpool(x)\n",
    "        # go through the fully connected layer\n",
    "        x = self.graph[-1](x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def test_preview_architecture():\n",
    "    # child model arch\n",
    "    sample_arch = []\n",
    "    # layer 0\n",
    "    sample_arch.append([0]) # op, c3\n",
    "    sample_arch.append([]) # skip, none\n",
    "    # layer 1\n",
    "    sample_arch.append([1]) # op, c5\n",
    "    sample_arch.append([1]) # skip=layer i + 1 input, l0=1\n",
    "    # layer 2\n",
    "    sample_arch.append([3]) # op, mp\n",
    "    sample_arch.append([0, 0]) # skip=layer i + 1 input, l0=0, l1=0\n",
    "    # layer 3\n",
    "    sample_arch.append([1]) # op, c5\n",
    "    sample_arch.append([1, 0, 1]) # skip=layer i + 1 input, l0=1, l1=0, l2=1\n",
    "    # layer 4\n",
    "    sample_arch.append([0]) # op, c3\n",
    "    sample_arch.append([0, 0, 0, 0]) # skip=layer i + 1 input, l0=0, l1=0, l2=0, l3=0\n",
    "    # layer 5\n",
    "    sample_arch.append([2]) # op, ap\n",
    "    sample_arch.append([0, 0, 0, 0, 0]) # skip=layer i + 1 input, l0=0, l1=0, l2=0, l3=0\n",
    "    print(sample_arch)\n",
    "    # instantiate a model\n",
    "    images = torch.rand([2, 3, 7, 7])\n",
    "    labels = torch.tensor([1, 2])\n",
    "    class_num = 5\n",
    "    num_layers = 6\n",
    "    out_channels = 2\n",
    "    child = ChildModel(class_num, num_layers, out_channels)\n",
    "    print(len(list(child.parameters())))\n",
    "    # print(list(child.parameters()))\n",
    "    print(len(child.graph))\n",
    "    print(child.graph)\n",
    "    y = child.model(images, sample_arch)\n",
    "    print(y.size())\n",
    "\n",
    "# ------------------\n",
    "# Run the test to get a preview of the model architecture\n",
    "# ------------------\n",
    "test_preview_architecture()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Child(nn.Module):\n",
    "    def __init__(self,\n",
    "               num_of_class,\n",
    "               num_of_layer=6,\n",
    "               out_channels=24,\n",
    "               batch_size=32,\n",
    "               lr=0.05,\n",
    "               gamma=0.1,\n",
    "               lr_cos_lmin=0.001,\n",
    "               lr_cos_Tmax=2,\n",
    "               l2_reg_lr=1e-4,\n",
    "               eval_period=100\n",
    "              ):\n",
    "        super(Child, self).__init__() \n",
    "        self.num_of_class = num_of_class # number of classification classes\n",
    "        self.num_of_layer = num_of_layer \n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.eval_period = eval_period\n",
    "\n",
    "        self.l2_reg_lr = l2_reg_lr\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.lr_cos_lmin = lr_cos_lmin\n",
    "        self.lr_cos_Tmax = lr_cos_Tmax\n",
    "        self.device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # build child model\n",
    "        self.net = ChildModel(num_of_class, num_of_layer, out_channels)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        if DEBUG: print('#param', len(list(self.net.parameters())))\n",
    "\n",
    "        self.optimizer = optim.SGD([{'params': self.net.parameters(), 'initial_lr': self.lr}], lr=self.lr, weight_decay=self.l2_reg_lr, momentum=0.9, nesterov=True)\n",
    "        \n",
    "        # learning rate scheduler - Not sure if it's necessary, basically it's a learning rate decay\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, self.lr_cos_Tmax, eta_min=self.lr_cos_lmin)\n",
    "    \n",
    "    def get_batch(self, images, classification_results, step):\n",
    "        # The original code takes batch as sequential data, not sure if we need to convert to random. If we ever encounter a problem, we can consider change it to random\n",
    "        batch_size = self.batch_size\n",
    "        batch_images = images[step * batch_size : (step + 1) * batch_size] \n",
    "        batch_classifications = classification_results[step * batch_size : (step + 1) * batch_size] \n",
    "        if DEBUG: print('get_batch', type(batch_images))\n",
    "        if DEBUG: print('get_batch', type(batch_classifications))\n",
    "\n",
    "        # augment data step - I comment it out because I'm not sure if it's necessary given the amount of data\n",
    "        # batch_images = augment(batch_images)\n",
    "\n",
    "        return batch_images, batch_classifications\n",
    "\n",
    "    def train_epoch(self, sample_arch, images, labels, epoch, train_step):    \n",
    "        running_loss = 0.0\n",
    "        if DEBUG: print('lr=', self.scheduler.get_lr())\n",
    "        for step in range(train_step): \n",
    "            batch_inputs, batch_classifications = self.get_batch(images, labels, step)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.net.model(batch_inputs, sample_arch)\n",
    "            loss = self.criterion(outputs, batch_classifications)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if step % self.eval_period == (self.eval_period - 1):\n",
    "                print('[%d, %5d], loss: %.3f' %\n",
    "                    (epoch + 1, step + 1, running_loss / self.eval_period))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        self.scheduler.step()\n",
    "\n",
    "    def valid_rl(self, sample_arch, images, labels):    \n",
    "        \"\"\"\n",
    "        validate a sampled child model on a random minibatch of validation set\n",
    "        \"\"\"\n",
    "        max_index = labels.size()[0] // self.batch_size\n",
    "        \n",
    "        batch_idx = torch.randint(max_index, (1,1))\n",
    "        batch_inputs, batch_classifications = self.get_batch(images, labels, batch_idx)\n",
    "\n",
    "        outputs = self.net.model(batch_inputs, sample_arch)\n",
    "        \n",
    "        value, idx = torch.topk(outputs, 1)\n",
    "        idx = idx.reshape((-1))\n",
    "        accuracy = (idx == batch_classifications).float().sum()\n",
    "        accuracy /= self.batch_size\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    def eval(self, sample_arch, images, labels):    \n",
    "        num_of_step = labels.size()[0] // self.batch_size\n",
    "        accuracy = 0\n",
    "        for i in range(num_of_step):\n",
    "            batch_inputs, batch_classifications = self.get_batch(images, labels, i)\n",
    "            outputs = self.net.model(batch_inputs, sample_arch)\n",
    "            _, idx = torch.topk(outputs, 1) # we can change it to argmax or softmax or guumbel softmax if necessary\n",
    "            idx = idx.reshape((-1))\n",
    "            accuracy += (idx == batch_classifications).float().sum()\n",
    "        accuracy /= (num_of_step * self.batch_size)\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "     \n",
    "\n",
    "def test_child():\n",
    "    # obtain datasets\n",
    "    t = time.time()\n",
    "    images, labels = read_data()\n",
    "    t = time.time() - t\n",
    "    print('read dataset consumes %.2f sec' % t)\n",
    "    # config of a model\n",
    "    class_num = 10\n",
    "    num_layers = 6\n",
    "    out_channels = 32\n",
    "    batch_size = 32\n",
    "    device = 'gpu'\n",
    "    epoch_num = 4\n",
    "    # sample a child model\n",
    "    sample_arch = []\n",
    "    # layer 0\n",
    "    sample_arch.append([0]) # op, c3\n",
    "    sample_arch.append([]) # skip, none\n",
    "    # layer 1\n",
    "    sample_arch.append([1]) # op, c5\n",
    "    sample_arch.append([1]) # skip=layer i + 1 input, l0=1\n",
    "    # layer 2\n",
    "    sample_arch.append([3]) # op, mp\n",
    "    sample_arch.append([0, 0]) # skip=layer i + 1 input, l0=0, l1=0\n",
    "    # layer 3\n",
    "    sample_arch.append([1]) # op, c5\n",
    "    sample_arch.append([1, 0, 1]) # skip=layer i + 1 input, l0=1, l1=0, l2=1\n",
    "    # layer 4\n",
    "    sample_arch.append([0]) # op, c3\n",
    "    sample_arch.append([0, 0, 0, 0]) # skip=layer i + 1 input, l0=0, l1=0, l2=0, l3=0\n",
    "    # layer 5\n",
    "    sample_arch.append([2]) # op, ap\n",
    "    sample_arch.append([0, 0, 0, 0, 0]) # skip=layer i + 1 input, l0=0, l1=0, l2=0, l3=0\n",
    "    print(sample_arch)\n",
    "    \n",
    "    # create a child\n",
    "    child = Child(images, labels, class_num, num_layers, out_channels, batch_size, device, epoch_num)\n",
    "    print(len(list(child.net.graph)))\n",
    "    # print(child.net.graph)\n",
    "    # train a child model\n",
    "    t = time.time()\n",
    "    child.train(sample_arch)\n",
    "    t = time.time() - t\n",
    "    print('training time %.2f sec' % t)\n",
    "\n",
    "    # # train another sample_arch\n",
    "    # sample_arch = []\n",
    "    # # layer 0\n",
    "    # sample_arch.append([1]) # op, c5\n",
    "    # sample_arch.append([]) # skip, none\n",
    "    # # layer 1\n",
    "    # sample_arch.append([0]) # op, c3\n",
    "    # sample_arch.append([1]) # skip=layer i + 1 input, l0=1\n",
    "    # # layer 2\n",
    "    # sample_arch.append([3]) # op, mp\n",
    "    # sample_arch.append([1, 0]) # skip=layer i + 1 input, l0=1, l1=0\n",
    "    # # layer 3\n",
    "    # sample_arch.append([0]) # op, c3\n",
    "    # sample_arch.append([1, 0, 1]) # skip=layer i + 1 input, l0=1, l1=0, l2=1\n",
    "    # # layer 4\n",
    "    # sample_arch.append([0]) # op, c3\n",
    "    # sample_arch.append([0, 1, 0, 1]) # skip=layer i + 1 input, l0=0, l1=1, l2=0, l3=1\n",
    "    # # layer 5\n",
    "    # sample_arch.append([2]) # op, ap\n",
    "    # sample_arch.append([0, 0, 0, 1, 1]) # skip=layer i + 1 input, l0=0, l1=0, l2=0, l3=1, l4=1\n",
    "    # print(sample_arch)\n",
    "\n",
    "    # print(len(list(child.net.graph)))\n",
    "    # print(child.net.graph)\n",
    "    # train a child model\n",
    "    t = time.time()\n",
    "    child.train(sample_arch)\n",
    "    t = time.time() - t\n",
    "    print('training time %.2f sec' % t)\n",
    "\n",
    "\n",
    "# the read data function is not implemented yet\n",
    "# test_child()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (lstm): StackLSTM(\n",
      "    (net): ModuleList(\n",
      "      (0-1): 2 x LSTMCell(32, 32)\n",
      "    )\n",
      "  )\n",
      "  (op_fc): Linear(in_features=32, out_features=4, bias=True)\n",
      "  (op_emb_lookup): Embedding(4, 32)\n",
      "  (skip_attn1): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (skip_attn2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (skip_attn3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class StackLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    StackLSTM class.\n",
    "    It describes a stacked LSTM which only \n",
    "    run a single step.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, lstm_num_layers=2):\n",
    "        # init\n",
    "        super(StackLSTM, self).__init__() # init the parent class of Net, i.e., nn.Module\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.net = self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        return nn.ModuleList([nn.LSTMCell(self.input_size, self.hidden_size) for _ in range(self.lstm_num_layers)])\n",
    "\n",
    "    def __call__(self, inputs, prev_h, prev_c):\n",
    "        next_h, next_c = [], []\n",
    "        for i, cell in enumerate(self.net):\n",
    "            x = inputs if i == 0 else next_h[-1]\n",
    "            cur_h, cur_c = cell(x, (prev_h[i], prev_c[i]))\n",
    "            next_h.append(cur_h)\n",
    "            next_c.append(cur_c)\n",
    "        return next_h, next_c\n",
    "\n",
    "class ControllerModel(nn.Module):\n",
    "    def __init__(self,\n",
    "               child_num_layers=6,\n",
    "               lstm_hidden_size=32,\n",
    "               lstm_num_layers=2,\n",
    "               num_operations=4,\n",
    "               temperature=5,\n",
    "               tanh_constant=2.5,\n",
    "               skip_target=0.4\n",
    "              ):\n",
    "        super(ControllerModel, self).__init__() # init the parent class of Net, i.e., nn.Module\n",
    "        self.child_num_layers = child_num_layers \n",
    "\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.num_operations = num_operations\n",
    "        self.temperature = temperature\n",
    "        self.tanh_constant = tanh_constant\n",
    "        self.skip_target = skip_target\n",
    "        self.device = DEVICE\n",
    "\n",
    "        self.net = self._build_net()\n",
    "        # add g_emb as a parameter to ControllerModel\n",
    "        # initialized by uniform distribution between -0.1 to 0.1\n",
    "        # 0 <= torch.rand < 1\n",
    "        g_emb_init = 0.2 * torch.rand(1,self.lstm_hidden_size) - 0.1\n",
    "        self.register_parameter(name='g_emb', param=torch.nn.Parameter(g_emb_init))\n",
    "\n",
    "        self.sample_arch = []\n",
    "        self.sample_entropy = []\n",
    "        self.sample_log_prob = []\n",
    "        self.sample_skip_count = []\n",
    "        self.sample_skip_penaltys = []\n",
    "        \n",
    "\n",
    "    def _build_net(self):\n",
    "        net = {}\n",
    "        net['lstm'] = StackLSTM(self.lstm_hidden_size, self.lstm_hidden_size, self.lstm_num_layers)\n",
    "        net['op_fc'] = nn.Linear(self.lstm_hidden_size, self.num_operations)\n",
    "        net['op_emb_lookup'] = nn.Embedding(self.num_operations, self.lstm_hidden_size)\n",
    "        net['skip_attn1'] = nn.Linear(self.lstm_hidden_size, self.lstm_hidden_size) # w_attn1 \n",
    "        net['skip_attn2'] = nn.Linear(self.lstm_hidden_size, self.lstm_hidden_size) # w_attn2\n",
    "        net['skip_attn3'] = nn.Linear(self.lstm_hidden_size, 1)                     # v_attn\n",
    "\n",
    "        if DEBUG:\n",
    "            for name in ['lstm', 'op_fc', 'op_emb_lookup', 'skip_attn1', 'skip_attn2', 'skip_attn3']:\n",
    "                param = list(net[name].parameters())\n",
    "                print(name)\n",
    "                for p in param:\n",
    "                    print(p.size())\n",
    "    \n",
    "        net = nn.ModuleDict(net)\n",
    "        \n",
    "        return net\n",
    "\n",
    "    def _op_sample(self, args):\n",
    "        \"\"\"\n",
    "        sample an op (it is a part of controller's forward)\n",
    "        Args: consisting of the following parts\n",
    "            inputs: input of op_sample\n",
    "            prev_h & prev_c: the hidden and cell states of the prev layer\n",
    "            arc_seq: architecture sequence\n",
    "            log_probs: all the log probabilities used for training (recall the gradient calculation of REINFORCE)\n",
    "            entropys: all the entropys used for training\n",
    "        Return:\n",
    "            x: output of the child model\n",
    "        \"\"\"\n",
    "        net = self.net\n",
    "        inputs, prev_h, prev_c, arc_seq, log_probs, entropys = args\n",
    " \n",
    "        next_h, next_c = net['lstm'](inputs, prev_h, prev_c)\n",
    "        prev_h, prev_c = next_h, next_c\n",
    "\n",
    "        logit = net['op_fc'](next_h[-1])    # h state of the last layer\n",
    "  \n",
    "        if self.temperature is not None:\n",
    "            logit /= self.temperature\n",
    "\n",
    "        if self.tanh_constant is not None:\n",
    "            logit = self.tanh_constant * torch.tanh(logit)\n",
    "\n",
    "        prob = f.softmax(logit, dim=1)\n",
    "        op_id = torch.multinomial(prob, 1) # logit = probs of each type of operation, 1 = sample a single op\n",
    "        op_id = op_id[0]\n",
    "\n",
    "        inputs = net['op_emb_lookup'](op_id.long())\n",
    "        log_prob = f.cross_entropy(logit, op_id)\n",
    "        entropy = log_prob * torch.exp(-log_prob)\n",
    "\n",
    "        if self.device == 'gpu':\n",
    "            op = op_id.cpu() # that's the line that I don't really understand\n",
    "        else:\n",
    "            op = op_id\n",
    "        op = int(op.data.numpy()) # to an int\n",
    "        op = [op] # to list\n",
    "        arc_seq.append(op)\n",
    "        log_probs.append(log_prob)\n",
    "        entropys.append(entropy)\n",
    "\n",
    "        return inputs, prev_h, prev_c, arc_seq, log_probs, entropys        \n",
    "\n",
    "    def _skip_sample(self, args):\n",
    "        \"\"\"\n",
    "        sample skip connections for layer_id (it is a part of controller's forward)\n",
    "        Args:\n",
    "            layer_id: layer count\n",
    "            inputs: input of op_sample\n",
    "            prev_h & prev_c: the hidden and cell states of the prev layer\n",
    "            arc_seq: architecture sequence\n",
    "            log_probs: all the log probabilities used for training (recall the gradient calculation of REINFORCE)\n",
    "            entropys: all the entropys used for training\n",
    "            archors & anchors_w_1: archor points and its weighed values\n",
    "            skip_targets & skip_penaltys & skip_count: used to enforce the sparsity of skip connections\n",
    "        Return:\n",
    "            all args except layer_id\n",
    "        \"\"\"    \n",
    "        layer_id, inputs, prev_h, prev_c, arc_seq, log_probs, entropys, anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count = args\n",
    "        net = self.net\n",
    "\n",
    "        next_h, next_c = net['lstm'](inputs, prev_h, prev_c)\n",
    "        prev_h, prev_c = next_h, next_c\n",
    "        if layer_id > 0:\n",
    "            # use attention mechanism to generate logits\n",
    "            # concate the weighed anchors\n",
    "            query = torch.cat(anchors_w_1, dim=0) \n",
    "            # attention 2 - fc\n",
    "            query = torch.tanh(net['skip_attn2'](next_h[-1]) + query)\n",
    "            # attention 3 - fc            \n",
    "            query = net['skip_attn3'](query)\n",
    "            # generate logit\n",
    "            logit = torch.cat([-query, query], dim=1)\n",
    "            # process logit with temperature\n",
    "            if self.temperature is not None:\n",
    "                logit /= self.temperature\n",
    "            # process logit with tanh and scale it\n",
    "            if self.temperature is not None:\n",
    "                logit = self.tanh_constant * torch.tanh(logit)\n",
    "            # calculate prob of skip (see NAS paper, Sec3.3)\n",
    "            skip_prob = torch.sigmoid(logit) # use sigmoid to convert skip to its prob\n",
    "            # sample skip connections using multinomial distribution sampler\n",
    "            skip = torch.multinomial(skip_prob, 1)  # 0 - used as an input, 1 - not an input\n",
    "            # calcualte kl as skip penalty\n",
    "            kl = skip_prob * torch.log(skip_prob / skip_targets) # calculate kl\n",
    "            kl = torch.sum(kl)\n",
    "            skip_penaltys.append(kl)\n",
    "            # cal log_prob and append it - used by REINFORCE to calculate gradients of controller (i.e., LSTM)\n",
    "            log_prob = f.cross_entropy(logit, skip.squeeze(dim=1))\n",
    "            log_probs.append(torch.sum(log_prob))\n",
    "            # cal entropys and append it\n",
    "            entropy = log_prob * torch.exp(-log_prob)\n",
    "            entropy = torch.sum(entropy)\n",
    "            entropys.append(entropy)\n",
    "            # update count of skips\n",
    "            skip_count.append(skip.sum())\n",
    "            # add skip to arc_seq\n",
    "            if self.device == 'gpu':\n",
    "                skip_cpu = skip.cpu()\n",
    "            else:\n",
    "                skip_cpu = skip\n",
    "            arc_seq.append(skip_cpu.squeeze(dim=1).data.numpy().tolist())\n",
    "            # generate inputs for the next time step\n",
    "            skip = torch.reshape(skip, (1, layer_id)) # reshape skip\n",
    "            cat_anchors = torch.cat(anchors, dim=0)\n",
    "            # skip = 1 x layer_id (layer_id > 0) \n",
    "            # cat_anchors = layer_id x lstm_size\n",
    "            inputs = torch.matmul(skip.float(), cat_anchors) \n",
    "            inputs /= (1.0 + torch.sum(skip))\n",
    "        else:\n",
    "            inputs = self.g_emb\n",
    "            if self.device == 'gpu':\n",
    "                inputs = inputs.cuda()\n",
    "            arc_seq.append([]) # no skip, use empty list to occupy the position\n",
    "        \n",
    "        # cal the\n",
    "        anchors.append(next_h[-1])\n",
    "        # cal attention 1\n",
    "        attn1 = net['skip_attn1'](next_h[-1])\n",
    "        anchors_w_1.append(attn1)\n",
    "\n",
    "        return inputs, prev_h, prev_c, arc_seq, log_probs, entropys, anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count\n",
    "\n",
    "    def net_sample(self):\n",
    "        \"\"\"\n",
    "        run (like forward) a controller model to sample an neural architecture\n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            \n",
    "        \"\"\"\n",
    "        # net sample\n",
    "        arc_seq = []\n",
    "        entropys = []\n",
    "        log_probs = []\n",
    "        # skip sample \n",
    "        anchors = []        # store hidden states of skip lstm; anchor = hidden states of skip lstm (i.e., layer_id)\n",
    "        anchors_w_1 = []    # store results of attention 1 (input=h, w_attn1)\n",
    "        skip_count = []\n",
    "        skip_penaltys = []\n",
    "\n",
    "        # determine the device used to run the model\n",
    "        if self.device == 'gpu': # check whether gpu is available or not\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else: device = 'cpu'\n",
    "        if DEBUG: print(device)\n",
    "        # move model to gpu\n",
    "        if self.device == 'gpu': # check whether gpu is available or not\n",
    "            self.net.to(device) # move net to gpu\n",
    "        # init inputs and states\n",
    "        # init prev cell states to zeros for each layer of the lstm\n",
    "        prev_c = [torch.zeros((1, self.lstm_hidden_size),device=device) for _ in range(self.lstm_num_layers)]\n",
    "        # init prev hidden states to zeros for each layer of the lstm\n",
    "        prev_h = [torch.zeros((1, self.lstm_hidden_size),device=device) for _ in range(self.lstm_num_layers)]\n",
    "        # inputs\n",
    "        inputs = self.g_emb\n",
    "        if self.device == 'gpu': # check whether gpu is available or not\n",
    "            inputs = inputs.cuda()\n",
    "        # skip_target = 0.4 = the prob of a layer used as an input of another layer\n",
    "        # 1 - skip_target = 0.6; the probability that this layer is not used as an input\n",
    "        skip_targets = torch.tensor([1.0 - self.skip_target, self.skip_target], dtype=torch.float, device=device)\n",
    "        \n",
    "\n",
    "        # sample an arch\n",
    "        for layer_id in range(self.child_num_layers):\n",
    "            arg_op_sample = [inputs, prev_h, prev_c, arc_seq, log_probs, entropys]\n",
    "            returns_op_sample = self._op_sample(arg_op_sample)\n",
    "            inputs, prev_h, prev_c, arc_seq, log_probs, entropys = returns_op_sample\n",
    "            arg_skip_sample = [layer_id, inputs, prev_h, prev_c, arc_seq, log_probs, entropys, \n",
    "                                anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count]\n",
    "            returns_skip_sample = self._skip_sample(arg_skip_sample)\n",
    "            inputs, prev_h, prev_c, arc_seq, log_probs, entropys, anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count = returns_skip_sample\n",
    "\n",
    "        # generate sample arch\n",
    "        # [[op], [skip]] * num_layer\n",
    "        self.sample_arch = arc_seq\n",
    "        if DEBUG: \n",
    "            print('sample_arch')\n",
    "            print('len:', len(self.sample_arch))\n",
    "            for idx, data in enumerate(self.sample_arch):\n",
    "                if idx % 2 == 0:\n",
    "                    print('-' * 15)\n",
    "                    print('layer:', idx)\n",
    "                    print('op:', data)\n",
    "                else:\n",
    "                    print('skip:', data)\n",
    "        # cal sample entropy\n",
    "        entropys = torch.stack(entropys)\n",
    "        self.sample_entropy = torch.sum(entropys)\n",
    "        if DEBUG: \n",
    "            print('sample_entropy: %.3f' % self.sample_entropy.item())\n",
    "            \n",
    "        # cal sample log_probs\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        self.sample_log_prob = torch.sum(log_probs)\n",
    "        if DEBUG: \n",
    "            print('sample_log_prob: %.3f' % self.sample_log_prob.item())\n",
    "            \n",
    "        # cal skip count\n",
    "        skip_count = torch.stack(skip_count)\n",
    "        self.sample_skip_count = torch.sum(skip_count)\n",
    "        if DEBUG: \n",
    "            print('sample_skip_count: %.0f' % self.sample_skip_count.item())\n",
    "            \n",
    "        # cal skip penaltys\n",
    "        skip_penaltys = torch.stack(skip_penaltys)\n",
    "        self.sample_skip_penaltys = torch.sum(skip_penaltys)\n",
    "        if DEBUG: \n",
    "            print('sample_skip_penaltys : %.3f' % self.sample_skip_penaltys.item())\n",
    "        \n",
    "\n",
    "def test_model():\n",
    "    ctrler = ControllerModel()\n",
    "    # param = list(ctrler.parameters())\n",
    "    # param_num = len(param)\n",
    "    # print(param_num)\n",
    "    # for i in range(param_num):\n",
    "    #     # print(p.size())\n",
    "    #     print(i, param[i].size())\n",
    "    print(ctrler.net)\n",
    "    ctrler.net_sample()\n",
    "    \n",
    "# ------------------\n",
    "# Testbench\n",
    "# ------------------\n",
    "if __name__ == '__main__':\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Controller class.\n",
    "    It describes how to train a controller\n",
    "        1) train\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               device='gpu',\n",
    "               lstm_size=32,\n",
    "               lstm_num_layers=2,\n",
    "               child_num_layers=6,\n",
    "               num_op=4,\n",
    "               train_step_num=50,\n",
    "               ctrl_batch_size=20,\n",
    "               opt_algo='adam',\n",
    "               lr_init=0.00035,\n",
    "               lr_gamma=0.1,\n",
    "               temperature=5,\n",
    "               tanh_constant=2.5,\n",
    "               entropy_weight=0.0001,\n",
    "               baseline_decay=0.999,\n",
    "               skip_target=0.4,\n",
    "               skip_weight=0.8\n",
    "              ):\n",
    "        \"\"\"\n",
    "        1. init params\n",
    "        2. create a graph which contains the sampled subgraph\n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__() # init the parent class of Net, i.e., nn.Module\n",
    "        # config of controller model\n",
    "        # child model\n",
    "        self.child_num_layers = child_num_layers # imgs of dataset\n",
    "        # ctrl model\n",
    "        self.lstm_size = lstm_size # labels of dataset \n",
    "        self.lstm_num_layers = lstm_num_layers # number of classes\n",
    "        self.num_op = num_op # \n",
    "        self.temperature = temperature\n",
    "        self.tanh_constant = tanh_constant\n",
    "        self.skip_target = skip_target\n",
    "        # ctrl training\n",
    "        self.ctrl_batch_size=ctrl_batch_size\n",
    "        self.opt_algo=opt_algo\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.train_step_num = train_step_num\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.baseline_decay = baseline_decay\n",
    "        self.skip_weight = skip_weight\n",
    "        # device\n",
    "        self.device = device\n",
    "        # # training parameters on cpu\n",
    "        if self.device == 'gpu':\n",
    "            self.reward = torch.zeros(1).cuda() # rewards of samples\n",
    "            self.baseline = torch.zeros(1).cuda() # base line\n",
    "            self.log_prob = torch.zeros(1).cuda() # log_probs of samples\n",
    "            self.entropy = torch.zeros(1).cuda() # entropys of samples\n",
    "            # self.skip_rate = torch.zeros(1) # skip_rates of samples\n",
    "            self.skip_penalty = torch.zeros(1).cuda() # skip_penaltys of samples\n",
    "            self.loss = torch.zeros(1).cuda() # loss\n",
    "        else:\n",
    "            self.reward = torch.zeros(1) # rewards of samples\n",
    "            self.baseline = torch.zeros(1) # base line\n",
    "            self.log_prob = torch.zeros(1) # log_probs of samples\n",
    "            self.entropy = torch.zeros(1) # entropys of samples\n",
    "            # self.skip_rate = torch.zeros(1) # skip_rates of samples\n",
    "            self.skip_penalty = torch.zeros(1) # skip_penaltys of samples\n",
    "            self.loss = torch.zeros(1) # loss\n",
    "        # training parameters on gpu\n",
    "        \n",
    "\n",
    "        # build controller\n",
    "        self.ctrl = ControllerModel(child_num_layers=child_num_layers,\n",
    "               lstm_size=lstm_size,\n",
    "               lstm_num_layers=lstm_num_layers,\n",
    "               num_op=num_op,\n",
    "               temperature=temperature,\n",
    "               tanh_constant=tanh_constant,\n",
    "               skip_target=skip_target,\n",
    "               device=device)\n",
    "        # Optimizer; use SGD\n",
    "        if DEBUG: print('#param', len(list(self.ctrl.parameters())))\n",
    "        # style: Adam\n",
    "        # self.optimizer = optim.Adam(self.ctrl.parameters(), lr=self.lr_init, betas=(0, 0.999)) # ENAS code sets beta1=0\n",
    "        self.optimizer = optim.Adam(self.ctrl.parameters(), lr=self.lr_init)\n",
    "        # self.optimizer = optim.SGD(self.net.parameters(), lr=self.lr_init, weight_decay=self.l2_reg, momentum=0.9, nesterov=True)\n",
    "        \n",
    "        # learning rate scheduler - not mentioned in paper, not use it first\n",
    "        # style: exponential decaying\n",
    "        # lr = gamma * lr for each epoch\n",
    "        # self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, self.lr_gamma)\n",
    "        # style: multistepLR\n",
    "        # decay lr every step_size epochs\n",
    "        # self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[1,2], gamma=0.1)\n",
    "        # style: stepLR; \n",
    "        # decay lr every step_size epochs\n",
    "        # self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=2, gamma=0.1)\n",
    "        # style: cosine\n",
    "        # self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, self.lr_cos_Tmax, eta_min=self.lr_cos_lmin)\n",
    "    \n",
    "    def train_epoch(self, child_model, images, labels, file):    \n",
    "        \"\"\"\n",
    "        train controller for an epoch\n",
    "        Procedure\n",
    "        for N train stpes:\n",
    "            for M child archs: - like obtain a batch of data\n",
    "                sample a child architecture\n",
    "                validate the sampled arch on a single minibatch of validation set\n",
    "                obtain reward\n",
    "                add weighed entropy to reward\n",
    "                update baseline\n",
    "                    exponential moving average of previous rewards\n",
    "                cal loss: add weighed skip penalty to loss\n",
    "            loss = avg( sample_log_prob * (reward - baseline) + skip_weight * skip_penaltys )\n",
    "            zero grads\n",
    "            cal grapds\n",
    "                loss.backward() = REINFORCE\n",
    "            update params of ctrl\n",
    "            \n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            \n",
    "        \"\"\"\n",
    "        # rewards = []\n",
    "        # skip_rates = []\n",
    "        # log_probs = []\n",
    "        # entropys = []\n",
    "        # skip_penaltys = []\n",
    "        for step in range(self.train_step_num):\n",
    "            # a single step of training\n",
    "            # sample a batch of child archs and obtain their metrics\n",
    "            if self.device == 'gpu':\n",
    "                loss = torch.zeros(self.ctrl_batch_size).cuda()\n",
    "            else:\n",
    "                loss = torch.zeros(self.ctrl_batch_size)\n",
    "            t_step = time.time()\n",
    "            for sample_cnt in range(self.ctrl_batch_size):\n",
    "                # sample a child arch\n",
    "                self.ctrl.net_sample()\n",
    "                # valid a sampled arch and obtain reward\n",
    "                self.reward = child_model.valid_rl(self.ctrl.sample_arch, images, labels) \n",
    "                # add weighed entropy to reward\n",
    "                self.entropy = self.ctrl.sample_entropy\n",
    "                self.reward += self.entropy_weight * self.entropy\n",
    "                # update baseline\n",
    "                with torch.no_grad():\n",
    "                    self.baseline = self.baseline + (1 - self.baseline_decay) * (self.reward - self.baseline)\n",
    "                # update loss\n",
    "                self.log_prob = self.ctrl.sample_log_prob\n",
    "                self.skip_penalty = self.ctrl.sample_skip_penaltys\n",
    "                loss[sample_cnt] = self.log_prob * (self.reward - self.baseline) + self.skip_weight * self.skip_penalty\n",
    "                # # cal skip rate and append it\n",
    "                # skip_rate = self.ctrl.sample_skip_count\n",
    "                # normalize = self.child_num_layers * (self.child_num_layers - 1) / 2\n",
    "                # skip_rate /= normalize\n",
    "                # skip_rates.append(skip_rate)\n",
    "            self.loss = loss.sum() / self.ctrl_batch_size # avg loss\n",
    "            # zero grads\n",
    "            self.optimizer.zero_grad()\n",
    "            # cal grads\n",
    "            # self.loss.backward(retain_graph=True)\n",
    "            self.loss.backward()\n",
    "            # update weights\n",
    "            self.optimizer.step()\n",
    "            # print(self.ctrl.net['op_fc'].weight.grad) # check grad is updated\n",
    "            # cal time consumed per step\n",
    "            t_step = time.time() - t_step\n",
    "            if step % 10 == 0:\n",
    "                print('step', step)\n",
    "                display_sample_arch(self.ctrl.sample_arch)\n",
    "                file.write('step:'+str(step))\n",
    "                print_sample_arch(self.ctrl.sample_arch, file)\n",
    "                print('time_per_step', t_step)\n",
    "                file.write('time_per_step'+str(t_step))\n",
    "\n",
    "    def get_op_portion(self):\n",
    "        \"\"\"\n",
    "        Count number of each type of ops in the sample arch\n",
    "            \n",
    "        Args: sample_arch\n",
    "        Return:\n",
    "        \"\"\"\n",
    "        op_counts = [0] * self.num_op\n",
    "        sample_arch = self.ctrl.sample_arch\n",
    "        for i in range(self.child_num_layers):\n",
    "            op_counts[sample_arch[2 * i][0]] += 1\n",
    "\n",
    "        return op_counts\n",
    "\n",
    "    def get_op_percent(self, op_histroy):\n",
    "        \"\"\"\n",
    "        Avg number of each type of ops in the sample arch\n",
    "            \n",
    "        Args: op_histroy\n",
    "        Return:\n",
    "        \"\"\"\n",
    "        num_samples = len(op_histroy)\n",
    "        op_histroy = np.stack(op_histroy)\n",
    "        op_history = np.sum(op_histroy, axis=0)\n",
    "        op_history = op_history / np.sum(op_history) # portion of each type of op\n",
    "\n",
    "        return op_history\n",
    "\n",
    "    def eval(self, child_model, arc_num, images, labels, file):\n",
    "        \"\"\"\n",
    "        evaluate controller using validating data set.\n",
    "        It samples several archs and validate them on \n",
    "        the whole validate set.\n",
    "            \n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            \n",
    "        \"\"\"\n",
    "        accuracy = []\n",
    "        arcs = []\n",
    "        op_percent = []\n",
    "        for _ in range(arc_num):\n",
    "            # sample a child arch\n",
    "            self.ctrl.net_sample()\n",
    "            arcs.append(self.ctrl.sample_arch)\n",
    "            # valid a sampled arch and obtain reward\n",
    "            eval_acc = child_model.eval(self.ctrl.sample_arch, images, labels) \n",
    "            accuracy.append(eval_acc)\n",
    "            # get the op analysis\n",
    "            op_percent.append(self.get_op_portion())\n",
    "        # obtain averaged op_history\n",
    "        op_percent = self.get_op_percent(op_percent)\n",
    "        # print to file  \n",
    "        # accuracy      \n",
    "        file.write('arch \\t accuracy\\n')    \n",
    "        for i, acc in enumerate(accuracy):\n",
    "            file.write('%d \\t %f\\n' % (i, acc)) \n",
    "        # arch       \n",
    "        for i, arc in enumerate(arcs):    \n",
    "            file.write('arch#: %d\\n' % i)\n",
    "            print_sample_arch(arc, file)\n",
    "        \n",
    "        return accuracy, op_percent\n",
    "\n",
    "    def derive_best_arch(self, child_model, arc_num, images, labels, file):\n",
    "        \"\"\"\n",
    "        derive the final child model using controller\n",
    "        procedure\n",
    "            1. sample 1000 archs\n",
    "            2. test them on test data set\n",
    "            3. select the one with highest accuracy as the best arch\n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            best_arch\n",
    "        \"\"\"\n",
    "        accuracy = []\n",
    "        arcs = []\n",
    "        best_arch = []\n",
    "        best_accuracy = 0\n",
    "        for _ in range(arc_num):\n",
    "            # sample a child arch\n",
    "            self.ctrl.net_sample()\n",
    "            arcs.append(self.ctrl.sample_arch)\n",
    "            # valid a sampled arch and obtain reward\n",
    "            eval_acc = child_model.eval(self.ctrl.sample_arch, images, labels) \n",
    "            accuracy.append(eval_acc)\n",
    "            # select the best arch\n",
    "            if eval_acc > best_accuracy:\n",
    "                best_accuracy = eval_acc\n",
    "                best_arch = self.ctrl.sample_arch\n",
    "        \n",
    "        # print to file  \n",
    "        # best accuracy and arc\n",
    "        file.write('best accuracy: %f\\n' % best_accuracy)\n",
    "        file.write('best arch \\n')\n",
    "        print_sample_arch(best_arch, file)\n",
    "        # accuracy    \n",
    "        file.write('-' * 30 + '\\n')   \n",
    "        file.write(' accuracies \\n')   \n",
    "        file.write('-' * 30 + '\\n')   \n",
    "        file.write('arch \\t accuracy\\n')    \n",
    "        for i, acc in enumerate(accuracy):\n",
    "            file.write('%d \\t %f\\n' % (i, acc)) \n",
    "        # arch     \n",
    "        file.write('-' * 30 + '\\n')   \n",
    "        file.write(' archs \\n')   \n",
    "        file.write('-' * 30 + '\\n')     \n",
    "        for i, arc in enumerate(arcs):    \n",
    "            file.write('arch#: %d\\n' % i)\n",
    "            print_sample_arch(arc, file)\n",
    "\n",
    "        return best_accuracy, best_arch\n",
    "\n",
    "def test_ctrl():\n",
    "    # obtain datasets\n",
    "    t = time.time()\n",
    "    images, labels = read_data()\n",
    "    t = time.time() - t\n",
    "    print('read dataset consumes %.2f sec' % t)\n",
    "    # config of a model\n",
    "    class_num = 10\n",
    "    child_num_layers = 6\n",
    "    out_channels = 32\n",
    "    batch_size = 32\n",
    "    device = 'gpu'\n",
    "    epoch_num = 4\n",
    "    # files to print sampled archs\n",
    "    child_filename = 'child_file.txt'\n",
    "    ctrl_filename = 'controller_file.txt'\n",
    "    child_file = open(child_filename, 'w')\n",
    "    ctrl_file = open(ctrl_filename, 'w')\n",
    "    # create a controller\n",
    "    ctrl = Controller(child_num_layers=child_num_layers)\n",
    "    # create a child, set epoch to 1; later this will be moved to an over epoch\n",
    "    child = Child(images, labels, class_num, child_num_layers, out_channels, batch_size, device, 1)\n",
    "    print(len(list(child.net.graph)))\n",
    "    # print(child.net.graph)\n",
    "    # train multiple epochs\n",
    "    for _ in range(epoch_num):\n",
    "        # sample an arch\n",
    "        ctrl.ctrl.net_sample()\n",
    "        sample_arch = ctrl.ctrl.sample_arch\n",
    "        print_sample_arch(sample_arch, child_file)\n",
    "        # train a child model\n",
    "        # t = time.time()\n",
    "        # child.train(sample_arch)\n",
    "        # t = time.time() - t\n",
    "        # print('child training time %.2f sec' % t)\n",
    "\n",
    "        # train controller\n",
    "        t = time.time()\n",
    "        ctrl.train(child, ctrl_file)\n",
    "        t = time.time() - t\n",
    "        print('ctrller training time %.2f sec' % t)\n",
    "\n",
    "# ------------------\n",
    "# Testbench\n",
    "# ------------------\n",
    "if __name__ == '__main__':\n",
    "    test_ctrl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
